# -*- coding: utf-8 -*-
"""Copy of finalvls

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tZk8NW-EiBY6S63uMmjzPPiH0823TAom
"""

!pip install torch==2.5.1

import torch
torchversion = torch.__version__

!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-{torchversion}.html
!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-{torchversion}.html
!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git

# 1. å¸è½½å½“å‰ PyTorch å’Œ torch-scatter ç­‰
!pip uninstall -y torch torchvision torchaudio torch-scatter torch-sparse torch-cluster torch-spline-conv

# 2. å®‰è£…å…¼å®¹ç‰ˆæœ¬ï¼ˆPyTorch 2.3.0 + CUDA 12.1ï¼‰
!pip install torch==2.3.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# 3. å®‰è£… PyG æ‰€éœ€ç»„ä»¶ï¼ˆtorch-scatter ç­‰ï¼‰
!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.3.0+cu121.html
!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.3.0+cu121.html
!pip install git+https://github.com/pyg-team/pytorch_geometric.git

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount("/content/gdrive")
# %cd /content/gdrive/MyDrive/3

from scipy.sparse import issparse
import torch
import numpy as np
import joblib

def extract_weight_array(w_dict):
    """å°†å­—å…¸ç±»å‹çš„æƒé‡å±•å¹³ä¸ºäºŒç»´æ•°ç»„"""
    if not isinstance(w_dict, dict):
        return None
    try:
        keys = sorted(w_dict.keys(), key=lambda x: int(x) if str(x).isdigit() else str(x))
        collected = []
        for k in keys:
            v = w_dict[k]
            if isinstance(v, list):
                arr = np.array(v)
                if arr.ndim == 1:
                    arr = arr[:, None]
                collected.append(arr)
        if not collected:
            return None
        return np.hstack(collected)
    except Exception as e:
        print(f"âš ï¸ extract_weight_array å¤±è´¥: {e}")
        return None


def build_samples(matrix_path, b_path, weight_path, source_tag):
    matrices = joblib.load(matrix_path)
    b_list = joblib.load(b_path)
    weight_list = joblib.load(weight_path)

    min_len = min(len(matrices), len(b_list), len(weight_list))

    if len(matrices) != len(b_list) or len(matrices) != len(weight_list):
        print(f"âš ï¸ [è­¦å‘Š] ä¸‰ä¸ªåˆ—è¡¨é•¿åº¦ä¸ä¸€è‡´ï¼ˆå°†ä½¿ç”¨å‰ {min_len} æ¡ï¼‰ï¼š")
        print(f"   matrix: {len(matrices)}, b: {len(b_list)}, weight: {len(weight_list)}")

    matrices = matrices[:min_len]
    b_list = b_list[:min_len]
    weight_list = weight_list[:min_len]

    samples = []
    skipped = 0

    for i in range(min_len):
        try:
            matrix = matrices[i].toarray() if issparse(matrices[i]) else matrices[i]
            b = b_list[i]
            w_raw = weight_list[i]

            weight = extract_weight_array(w_raw)
            if weight is None:
                skipped += 1
                continue

            samples.append((matrix, b, weight, source_tag))
        except Exception as e:
            print(f"[è·³è¿‡] ç¬¬ {i} ä¸ªæ ·æœ¬å¤„ç†å¤±è´¥: {e}")
            skipped += 1
            continue

    print(f"âœ… {source_tag} åŠ è½½æˆåŠŸ: {len(samples)} ä¸ªæœ‰æ•ˆæ ·æœ¬ï¼Œè·³è¿‡ {skipped} ä¸ªæ— æ•ˆæ ·æœ¬")
    return samples

data_8 = build_samples("sparse8_matrix_list.pkl", "sparse8_b_list.pkl", "weights_list_8.pkl", "8")
data_9 = build_samples("sparse9_matrix_list.pkl", "sparse9_b_list.pkl", "weights_list_9.pkl", "9")
data_10 = build_samples("sparse10_matrix_list.pkl", "sparse10_b_list.pkl", "weights_list_10.pkl", "10")

import joblib
import numpy as np
from scipy.sparse import issparse

# åŠ è½½è·¯å¾„
matrix_path = "sparse_pkl_data_matrix.pkl"
b_path = "sparse_pkl_data_b.pkl"
w_path = "sparse_pkl_data_w.pkl"

# åŠ è½½æ–‡ä»¶
matrices = joblib.load(matrix_path)
b_list = joblib.load(b_path)
w_list = joblib.load(w_path)

def flatten_weight_array(weight_array):
    """å°†ä¸‰ç»´æ•°ç»„å±•å¹³ä¸ºäºŒç»´ (N, d)"""
    if isinstance(weight_array, np.ndarray) and weight_array.ndim == 3:
        return weight_array.reshape(-1, weight_array.shape[-1])
    return None

samples = []
skipped = 0

min_len = min(len(matrices), len(b_list), len(w_list))

for i in range(min_len):
    try:
        matrix = matrices[i].toarray() if issparse(matrices[i]) else matrices[i]
        b = b_list[i]
        w_raw = w_list[i]
        weight = flatten_weight_array(w_raw)
        if weight is None:
            skipped += 1
            continue
        samples.append((matrix, b, weight, "test"))
    except Exception as e:
        print(f"[è·³è¿‡] ç¬¬ {i} ä¸ªæ ·æœ¬å‡ºé”™: {e}")
        skipped += 1
        continue

print(f"âœ… æ€»è®¡ç”Ÿæˆ {len(samples)} ä¸ªæ ·æœ¬ï¼Œè·³è¿‡ {skipped} ä¸ª")

# ä¿å­˜ä¸º .pkl æ ¼å¼
joblib.dump(samples, "test.pkl")
print("âœ… å·²ä¿å­˜ä¸º test.pkl")
import torch
import joblib

samples = joblib.load("test.pkl")
torch.save(samples, "test.pt")

import torch
import joblib
from torch_geometric.data import Data
from torch_geometric.loader import DataLoader
import numpy as np

# åŠ è½½ test.ptï¼ˆä½ ä¹‹å‰ä¿å­˜çš„ test.pt æ–‡ä»¶ï¼‰
test_tuples = torch.load("test.pt", map_location=torch.device("cpu"))  # å¼ºåˆ¶åŠ è½½ä¸º CPU

# å·¥å…·å‡½æ•°ï¼šå°†æ•°æ®è½¬æ¢ä¸º PyG çš„ Data æ ¼å¼
def tuple_to_data(tuples):
    data_list = []
    for A, b, w, tag in tuples:
        A_tensor = torch.tensor(A, dtype=torch.float)
        b_tensor = torch.tensor(b, dtype=torch.float).view(-1, 1)
        w_tensor = torch.tensor(w, dtype=torch.float)

        edge_index = (A_tensor != 0).nonzero(as_tuple=False).t().contiguous()
        edge_weight = A_tensor[edge_index[0], edge_index[1]]

        data = Data(x=w_tensor, edge_index=edge_index, edge_attr=edge_weight, y=b_tensor)
        data_list.append(data)
    return data_list

# è½¬æ¢ä¸º Data åˆ—è¡¨å¹¶æ„é€  DataLoader
test_dataset = tuple_to_data(test_tuples)
test_loader = DataLoader(test_dataset, batch_size=32)

print(f"âœ… Test dataset loaded with {len(test_dataset)} samples.")

import torch
import joblib

samples = joblib.load("test.pkl")
torch.save(samples, "test.pt")

import random
import torch

# åˆå¹¶æ•°æ®
all_data = data_8 + data_9 + data_10
random.shuffle(all_data)  # æ‰“ä¹±é¡ºåº

# åˆ’åˆ†è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†ï¼ˆ8:1:1ï¼‰
total_len = len(all_data)
train_len = int(total_len * 0.8)
val_len = int(total_len * 0.1)

train_data = all_data[:train_len]
val_data = all_data[train_len:train_len + val_len]
test_data = all_data[train_len + val_len:]

# æ ¹æ® source_tag å†æ¬¡åˆ’åˆ†æµ‹è¯•é›†
test_8 = [d for d in test_data if d[3] == "8"]
test_9 = [d for d in test_data if d[3] == "9"]
test_10 = [d for d in test_data if d[3] == "10"]

# è¾“å‡ºä¿¡æ¯
print(f"ğŸ“¦ Train: {len(train_data)}")
print(f"ğŸ“¦ Val: {len(val_data)}")
print(f"ğŸ“¦ Test: {len(test_data)}")
print(f"   â”œâ”€â”€ test_8: {len(test_8)}")
print(f"   â”œâ”€â”€ test_9: {len(test_9)}")
print(f"   â””â”€â”€ test_10: {len(test_10)}")

# å¦‚éœ€ä¿å­˜ï¼Œå¯ä½¿ç”¨ä»¥ä¸‹ä»£ç ï¼ˆå¦‚æœä½ æƒ³ä¿ç•™ï¼‰
# torch.save(train_data, "train.pt")
# torch.save(val_data, "val.pt")
# torch.save(test_data, "test.pt")
# torch.save(test_8, "test_8.pt")
# torch.save(test_9, "test_9.pt")
# torch.save(test_10, "test_10.pt")
import torch

# ä¿å­˜åˆ°å½“å‰å·¥ä½œç›®å½•
torch.save(train_data, "train.pt")
torch.save(val_data, "val.pt")
torch.save(test_data, "test.pt")
torch.save(test_8, "test_8.pt")
torch.save(test_9, "test_9.pt")
torch.save(test_10, "test_10.pt")

print("âœ… æ‰€æœ‰æ–‡ä»¶ä¿å­˜å®Œæˆã€‚")

import os

for fname in ["train.pt", "val.pt", "test.pt", "test_8.pt", "test_9.pt", "test_10.pt"]:
    print(f"{fname}: {'å­˜åœ¨ âœ…' if os.path.exists(fname) else 'ä¸å­˜åœ¨ âŒ'}")

import torch
from torch_geometric.data import Data
from scipy.sparse import issparse

def tuple_to_data(samples, max_len=30):
    data_list = []
    qn = max_len // 3

    for i, (matrix, b, weight, tag) in enumerate(samples):
        try:
            # 1. ç¨€ç–çŸ©é˜µè½¬ç¨ å¯†
            if issparse(matrix):
                matrix = matrix.toarray()
            matrix = torch.tensor(matrix, dtype=torch.float32, device='cpu')
            row, col = torch.where(matrix != 0)
            edge_index = torch.stack([row, col], dim=0).long()
            edge_attr = matrix[row, col].unsqueeze(1)

            # 2. å¤„ç† bï¼ˆæ”¯æŒåµŒå¥—å­—ç¬¦ä¸²æˆ–æ•°å­—ï¼‰
            flat_b = []
            for val in b:
                if isinstance(val, (list, tuple)):
                    flat_b.extend([float(v) for v in val])
                else:
                    flat_b.append(float(val))
            x = torch.tensor([[v] for v in flat_b], dtype=torch.float32, device='cpu')

            # 3. æ ‡ç­¾å¤„ç†
            y_raw = torch.tensor(weight, dtype=torch.float32, device='cpu').flatten()
            if y_raw.shape[0] < max_len:
                y_raw = torch.cat([y_raw, torch.zeros(max_len - y_raw.shape[0], device='cpu')])
            y = y_raw[:max_len].view(qn, 3)

            # 4. æ„å»ºå›¾æ•°æ®
            data = Data(
                edge_index=edge_index,
                edge_attr=edge_attr,
                x=x,
                y=y
            )
            data_list.append(data)

        except Exception as e:
            print(f"[è·³è¿‡] ç¬¬ {i} ä¸ªæ ·æœ¬è½¬æ¢å¤±è´¥: {e}")
            print(f"  â–¶ï¸ bå†…å®¹ç¤ºä¾‹: {str(b)[:100]}")
            continue

    return data_list

from torch_geometric.data import Data

def force_data_to_cpu(dataset):
    for i in range(len(dataset)):
        data = dataset[i]
        new_data = Data()
        for key in data.keys():  # âœ… æ³¨æ„è¿™é‡ŒåŠ ä¸Šæ‹¬å· keys()
            item = data[key]
            if isinstance(item, torch.Tensor):
                new_data[key] = item.cpu()
            else:
                new_data[key] = item
        dataset[i] = new_data

# ç¬¬ä¸€æ­¥ï¼šåŠ è½½åŸå§‹æ•°æ®
train_tuples = torch.load("train.pt")
val_tuples = torch.load("val.pt")
test_8_tuples = torch.load("test_8.pt")

# ç¬¬äºŒæ­¥ï¼šè½¬æ¢ä¸ºå›¾ç»“æ„ï¼ˆPyGçš„Dataå¯¹è±¡ï¼‰
train_dataset = tuple_to_data(train_tuples)
val_dataset = tuple_to_data(val_tuples)
test_8_dataset = tuple_to_data(test_8_tuples)


train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)


# ç¬¬ä¸‰æ­¥ï¼šDataLoader åŠ è½½
from torch_geometric.loader import DataLoader
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)
test_8_loader = DataLoader(test_8_dataset, batch_size=32)

from torch_geometric.loader import DataLoader

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64)
test_loader_8 = DataLoader(test_8, batch_size=64)

from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout, Tanh
import torch.nn.functional as F
from torch_geometric.nn import GATConv, global_mean_pool, MessagePassing, conv
from torch_geometric.utils import add_self_loops, degree
from torch_scatter import scatter_add
import copy
from torch_sparse import sum as sparsesum
from torch_sparse import mul
from torch_sparse import SparseTensor


def directed_norm(adj):
  in_deg = sparsesum(adj, dim=0)
  in_deg_inv_sqrt = in_deg.pow_(-0.5)
  in_deg_inv_sqrt.masked_fill_(in_deg_inv_sqrt == float("inf"), 0.0)

  out_deg = sparsesum(adj, dim=1)
  out_deg_inv_sqrt = out_deg.pow_(-0.5)
  out_deg_inv_sqrt.masked_fill_(out_deg_inv_sqrt == float("inf"), 0.0)
  adj = mul(adj, out_deg_inv_sqrt.view(-1, 1))
  adj = mul(adj, in_deg_inv_sqrt.view(1, -1))
  return adj


class GCNConv_lap(MessagePassing):
  def __init__(self, in_channels, out_channels):
    self.in_channels = in_channels
    self.out_channels = out_channels
    super(GCNConv_lap, self).__init__(aggr='add')
    self.lin = torch.nn.Linear(in_channels, out_channels)

  def forward(self, x, edge_index, edge_weight):
    edge_index_self_loop, _ = add_self_loops(edge_index, num_nodes=x.size(0))
    edge_weight_self_loop = torch.cat((edge_weight.flatten(), torch.ones(x.size(0)).to("cuda")), dim=0)

    x = self.lin(x)

    row, col = edge_index_self_loop
    deg = scatter_add(torch.abs(edge_weight_self_loop), col, dim=0, dim_size=x.size(0))
    deg_inv_sqrt = deg.pow(-0.5)
    norm = deg_inv_sqrt[row] * torch.abs(edge_weight_self_loop) * deg_inv_sqrt[col]

    return self.propagate(edge_index_self_loop, x=x, norm=norm, edge_weight_self_loop=edge_weight_self_loop)

  def message(self, x_i, x_j, norm, edge_weight_self_loop):
    x_j = torch.sign(edge_weight_self_loop).view(-1,1)*x_j
    return (norm.view(-1, 1) * x_j)


class DGNNConv(torch.nn.Module):
  def __init__(
      self,
      mconv: conv.MessagePassing,
      alpha: float = 0.5,
      root_weight: bool = True,
  ):
    super().__init__()
    self.alpha = alpha
    self.root_weight = root_weight
    self.conv_in = copy.deepcopy(mconv)
    self.conv_out = copy.deepcopy(mconv)

    self.reset_parameters()

  def reset_parameters(self):
    self.conv_in.reset_parameters()
    self.conv_out.reset_parameters()

  def forward(self, x, edge_index, edge_attr):
    row, col = edge_index
    num_nodes = x.shape[0]
    adj = SparseTensor(row=row, col=col, sparse_sizes=(num_nodes, num_nodes))
    self.adj_norm = directed_norm(adj)
    adj_t = SparseTensor(row=col, col=row, sparse_sizes=(num_nodes, num_nodes))
    self.adj_t_norm = directed_norm(adj_t)

    x_in = self.conv_in(self.adj_norm @ x, edge_index, edge_attr)
    x_out = self.conv_out(self.adj_t_norm @ x, edge_index.flip(0), edge_attr)

    out = self.alpha * x_out + (1 - self.alpha) * x_in

    return out


import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Linear, ReLU
from torch_geometric.nn import global_mean_pool, GATConv, GCNConv as GCNConv_lap, MessagePassing
import copy

class DGNNConv(nn.Module):
    def __init__(self, mconv: MessagePassing, alpha: float = 0.5, root_weight: bool = True):
        super().__init__()
        self.alpha = alpha
        self.root_weight = root_weight
        self.conv_in = copy.deepcopy(mconv)
        self.conv_out = copy.deepcopy(mconv)

    def reset_parameters(self):
        self.conv_in.reset_parameters()
        self.conv_out.reset_parameters()

    def forward(self, x, edge_index, edge_attr):
        x_in = self.conv_in(x, edge_index, edge_attr)
        x_out = self.conv_out(x, edge_index.flip(0), edge_attr)
        return self.alpha * x_out + (1 - self.alpha) * x_in

class DGNN(nn.Module):
    def __init__(self, n_x, qn, dim_h=256, alpha=0.5):
        super().__init__()
        self.qn = qn
        self.output_dim = qn * 3

        self.hidden_channels_gcn = dim_h * 2
        self.hidden_channels_gat = self.hidden_channels_gcn * 2

        self.conv1 = DGNNConv(GCNConv_lap(n_x, self.hidden_channels_gcn), alpha)
        self.conv2 = DGNNConv(GCNConv_lap(self.hidden_channels_gcn, self.hidden_channels_gcn), alpha)

        self.gat_conv1 = GATConv(self.hidden_channels_gcn, self.hidden_channels_gat)
        self.gat_conv2 = GATConv(self.hidden_channels_gat, self.hidden_channels_gat)

        self.out = nn.Sequential(
            Linear(self.hidden_channels_gat, self.hidden_channels_gat),
            ReLU(),
            Linear(self.hidden_channels_gat, self.output_dim)
        )

    def forward(self, x, edge_index, edge_weight, batch):
        x = F.relu(self.conv1(x, edge_index, edge_weight))
        x = F.relu(self.conv2(x, edge_index, edge_weight))
        x = F.relu(self.gat_conv1(x, edge_index, edge_weight))
        x = F.relu(self.gat_conv2(x, edge_index, edge_weight))
        x = global_mean_pool(x, batch)
        x = self.out(x)
        return x.view(-1, self.qn, 3)   # åŠ¨æ€ reshape

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
from torch_geometric.loader import DataLoader
from torch_geometric.data import Data

# âœ… å·¥å…·å‡½æ•°ï¼šå¼ºåˆ¶å°†æ•°æ®ä» GPU è½¬åˆ° CPU
def force_data_to_cpu(dataset):
    for i in range(len(dataset)):
        data = dataset[i]
        new_data = Data()
        for key in data.keys():
            item = data[key]
            if isinstance(item, torch.Tensor):
                new_data[key] = item.cpu()
            else:
                new_data[key] = item
        dataset[i] = new_data

# âœ… å¼ºåˆ¶è®­ç»ƒæ•°æ®å’ŒéªŒè¯æ•°æ®éƒ½åœ¨ CPU ä¸Š
force_data_to_cpu(train_dataset)
force_data_to_cpu(val_dataset)

# âœ… æ„å»º DataLoader
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)

# âœ… è®¾å¤‡é€‰æ‹©
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# âœ… åŠ¨æ€ç¡®å®šè¾“å…¥ç»´åº¦å’Œ qn
example = train_dataset[0]
n_x = example.x.shape[1]
qn = example.y.shape[0]  # å‡è®¾ä½ ç”¨äº† max_len=30ï¼Œ3åˆ—ï¼Œqn=10

# âœ… åˆå§‹åŒ–æ¨¡å‹ï¼ˆä½ è‡ªå·±æä¾›çš„ DGNN å®ç°ï¼‰
model = DGNN(n_x=n_x, qn=qn, dim_h=256, alpha=0.5).to(device)

# âœ… æ¨¡å‹è¾“å‡ºæ£€æŸ¥
sample = example
model.eval()
with torch.no_grad():
    pred = model(
        sample.x.to(device),
        sample.edge_index.to(device),
        sample.edge_attr.to(device),
        torch.zeros(sample.x.shape[0], dtype=torch.long).to(device)  # dummy batch
    )
    print("âœ… Model output shape:", pred.shape)  # åº”ä¸º [1, qn, 3]

# âœ… æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨
loss_fn = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.005)
scheduler = lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)

# âœ… è®­ç»ƒ
n_epochs = 50
best_val_loss = float("inf")
save_path = "best_model.pth"

for epoch in range(n_epochs):
    model.train()
    train_losses = []

    for batch in train_loader:
        batch = batch.to(device)
        optimizer.zero_grad()

        pred = model(
            batch.x.to(device),
            batch.edge_index.to(device),
            batch.edge_attr.to(device),
            batch.batch.to(device)
        )
        y = batch.y.to(device).view(pred.size(0), qn, 3)

        loss = loss_fn(pred, y)
        loss.backward()
        optimizer.step()
        train_losses.append(loss.item())

    train_mse = sum(train_losses) / len(train_losses)
    print(f"ğŸ“˜ Epoch {epoch} | Train MSE: {train_mse:.6f}")

    # âœ… éªŒè¯
    model.eval()
    val_losses = []
    with torch.no_grad():
        for batch in val_loader:
            batch = batch.to(device)
            pred = model(
                batch.x.to(device),
                batch.edge_index.to(device),
                batch.edge_attr.to(device),
                batch.batch.to(device)
            )
            y = batch.y.to(device).view(pred.size(0), qn, 3)
            loss = loss_fn(pred, y)
            val_losses.append(loss.item())

    val_mse = sum(val_losses) / len(val_losses)
    print(f"ğŸ“— Epoch {epoch} | Val MSE: {val_mse:.6f}")

    if val_mse < best_val_loss:
        best_val_loss = val_mse
        torch.save(model.state_dict(), save_path)
        print(f"âœ… Best model saved at epoch {epoch} | Val MSE: {best_val_loss:.6f}")

    scheduler.step()

test_8_tuples = torch.load("test_8.pt")
test_9_tuples = torch.load("test_9.pt")
test_10_tuples = torch.load("test_10.pt")

import torch
from torch_geometric.loader import DataLoader
from torch_geometric.data import Data
from scipy.sparse import issparse

def tuple_to_data(samples, max_len=30):
    data_list = []
    for i, (matrix, b, weight, tag) in enumerate(samples):
        try:
            # 1. ç¨€ç–çŸ©é˜µè½¬ç¨ å¯†
            if issparse(matrix):
                matrix = matrix.toarray()
            matrix = torch.tensor(matrix, dtype=torch.float32)
            row, col = torch.where(matrix != 0)
            edge_index = torch.stack([row, col], dim=0).long()
            edge_attr = matrix[row, col].unsqueeze(1)

            # 2. èŠ‚ç‚¹ç‰¹å¾ b å¤„ç†
            flat_b = []
            for val in b:
                if isinstance(val, (list, tuple)):
                    flat_b.extend([float(v) for v in val])
                else:
                    flat_b.append(float(val))
            x = torch.tensor([[v] for v in flat_b], dtype=torch.float32)

            # 3. æ ‡ç­¾å¤„ç†
            y_raw = torch.tensor(weight, dtype=torch.float32).flatten()
            y = y_raw[:max_len] if y_raw.shape[0] >= max_len else torch.cat([
                y_raw, torch.zeros(max_len - y_raw.shape[0])
            ])
            qn = max_len // 3
            y = y.view(qn, 3)

            # 4. æ„å»ºå›¾
            data = Data(
                edge_index=edge_index.cpu(),
                edge_attr=edge_attr.cpu(),
                x=x.cpu(),
                y=y.cpu()
            )
            data_list.append(data)

        except Exception as e:
            print(f"[è·³è¿‡] ç¬¬ {i} ä¸ªæ ·æœ¬å¤±è´¥: {e}")
            continue
    return data_list

def predict_on_test(test_tuples, true_qn, model_path="best_model.pth", save_path="test_pred.pt"):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # 1. æ„é€ å›¾æ•°æ®é›†
    test_dataset = tuple_to_data(test_tuples, max_len=30)
    if len(test_dataset) == 0:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆæµ‹è¯•æ ·æœ¬ï¼Œè·³è¿‡é¢„æµ‹ã€‚")
        return

    test_loader = DataLoader(test_dataset, batch_size=32)

    # 2. åˆå§‹åŒ–æ¨¡å‹
    example = test_dataset[0]
    n_x = example.x.shape[1]
    model = DGNN(n_x=n_x, qn=10, dim_h=256, alpha=0.5).to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()

    # 3. æ‰¹é‡æ¨ç†
    all_preds = []
    with torch.no_grad():
        for batch in test_loader:
            batch = batch.to(device)
            out = model(
                batch.x.to(device),
                batch.edge_index.to(device),
                batch.edge_attr.to(device),
                batch.batch.to(device)
            )  # è¾“å‡º: (B, 10, 3)

            out = out[:, :true_qn, :]  # âš ï¸ è£å‰ªåˆ°æŒ‡å®šé•¿åº¦
            all_preds.append(out.cpu())

    # 4. æ‹¼æ¥ä¿å­˜
    final_tensor = torch.cat(all_preds, dim=0)  # (N, true_qn, 3)
    torch.save(final_tensor, save_path)
    print(f"âœ… å·²ä¿å­˜é¢„æµ‹ç»“æœåˆ°: {save_path} | å½¢çŠ¶: {final_tensor.shape}")
# å‡è®¾ä½ å·²ç»å‡†å¤‡å¥½äº† test_8_tuples ç­‰
predict_on_test(test_8_tuples, true_qn=8, save_path="test_8_pred.pt")
predict_on_test(test_9_tuples, true_qn=9, save_path="test_9_pred.pt")
predict_on_test(test_10_tuples, true_qn=10, save_path="test_10_pred.pt")

import torch

tensor = torch.load("test_9_pred.pt", map_location="cpu")
print(tensor.shape)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
from torch_geometric.loader import DataLoader
from torch_geometric.data import Data



example = test_dataset[0]
n_x = example.x.shape[1]
qn = example.y.shape[0] if hasattr(example, "y") else 10  # è‹¥æ—  yï¼Œå¯æ‰‹åŠ¨æŒ‡å®š
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = DGNN(n_x=n_x, qn=qn, dim_h=256, alpha=0.5).to(device)
model.load_state_dict(torch.load("best_model.pt", map_location=device))
model.eval()

# Step 3: æ¨ç†
all_preds = []
with torch.no_grad():
    for batch in test_loader:
        batch = batch.to(device)
        out = model(
            batch.x,
            batch.edge_index,
            batch.edge_attr,
            batch.batch
        )  # è¾“å‡ºå½¢çŠ¶: (B, qn, 3)
        out = out[:, :qn, :]  # è‹¥æ¨¡å‹è¾“å‡ºå¤§äºå®é™… qnï¼Œåˆ™è£å‰ª
        all_preds.append(out.cpu())

# Step 4: ä¿å­˜é¢„æµ‹ç»“æœ
final_tensor = torch.cat(all_preds, dim=0)  # (N, qn, 3)
torch.save(final_tensor, "test_pred.pt")
print(f"âœ… æ¨ç†å®Œæˆï¼Œé¢„æµ‹ç»“æœå·²ä¿å­˜è‡³ test_pred.ptï¼Œå½¢çŠ¶ä¸º: {final_tensor.shape}")

from google.colab import drive
drive.mount('/content/drive')

# å¦‚æœä½ å·²ç»çŸ¥é“æ–‡ä»¶åœ¨ Google Drive ä¸­çš„è·¯å¾„
file_path = "/content/drive/MyDrive/3/best_model.pt"  # æ ¹æ®å®é™…è·¯å¾„è°ƒæ•´

# åŠ è½½æ¨¡å‹
model.load_state_dict(torch.load(file_path, map_location=device))

import torch
import time
from torch_geometric.loader import DataLoader
from torch_geometric.data import Data
from scipy.sparse import issparse

def tuple_to_data(samples, max_len=30):
    data_list = []
    for i, (matrix, b, weight, tag) in enumerate(samples):
        try:
            if issparse(matrix):
                matrix = matrix.toarray()
            matrix = torch.tensor(matrix, dtype=torch.float32)
            row, col = torch.where(matrix != 0)
            edge_index = torch.stack([row, col], dim=0).long()
            edge_attr = matrix[row, col].unsqueeze(1)

            flat_b = []
            for val in b:
                if isinstance(val, (list, tuple)):
                    flat_b.extend([float(v) for v in val])
                else:
                    flat_b.append(float(val))
            x = torch.tensor([[v] for v in flat_b], dtype=torch.float32)

            y_raw = torch.tensor(weight, dtype=torch.float32).flatten()
            y = y_raw[:max_len] if y_raw.shape[0] >= max_len else torch.cat([
                y_raw, torch.zeros(max_len - y_raw.shape[0])
            ])
            qn = max_len // 3
            y = y.view(qn, 3)

            data = Data(
                edge_index=edge_index.cpu(),
                edge_attr=edge_attr.cpu(),
                x=x.cpu(),
                y=y.cpu()
            )
            data_list.append(data)

        except Exception as e:
            print(f"[è·³è¿‡] ç¬¬ {i} ä¸ªæ ·æœ¬å¤±è´¥: {e}")
            continue
    return data_list

def predict_on_test(test_tuples, true_qn, model_path="best_model.pth", save_path="test_pred.pt"):
    import time
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    test_dataset = tuple_to_data(test_tuples, max_len=30)
    if len(test_dataset) == 0:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆæµ‹è¯•æ ·æœ¬ï¼Œè·³è¿‡é¢„æµ‹ã€‚")
        return

    test_loader = DataLoader(test_dataset, batch_size=32)

    example = test_dataset[0]
    n_x = example.x.shape[1]
    model = DGNN(n_x=n_x, qn=10, dim_h=256, alpha=0.5).to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()

    all_preds = []

    start_time = time.time()  # â±ï¸å¼€å§‹è®¡æ—¶
    with torch.no_grad():
        for batch in test_loader:
            batch = batch.to(device)
            out = model(
                batch.x.to(device),
                batch.edge_index.to(device),
                batch.edge_attr.to(device),
                batch.batch.to(device)
            )
            out = out[:, :true_qn, :]
            all_preds.append(out.cpu())
    end_time = time.time()  # â±ï¸ç»“æŸè®¡æ—¶

    final_tensor = torch.cat(all_preds, dim=0)
    torch.save(final_tensor, save_path)

    print(f"âœ… å·²ä¿å­˜é¢„æµ‹ç»“æœåˆ°: {save_path} | å½¢çŠ¶: {final_tensor.shape}")
    print(f"ğŸ•’ æ¨ç†è€—æ—¶: {end_time - start_time:.4f} ç§’")

# ç¤ºä¾‹è°ƒç”¨
predict_on_test(test_8_tuples, true_qn=8, save_path="test_8_pred.pt")
predict_on_test(test_9_tuples, true_qn=9, save_path="test_9_pred.pt")
predict_on_test(test_10_tuples, true_qn=10, save_path="test_10_pred.pt")